---
name: "jamba"

config_file: |
  mmap: true
  backend: "llama-cpp"
  template:
    chat_message: |
      <|im_start|>{{if eq .RoleName "tool" }}user{{else}}{{ .RoleName }}{{end}}
      {{ if eq .RoleName "tool" -}}
      <tool_response>
      {{ end -}}
      {{ if .Content -}}
      {{.Content }}
      {{ end -}}
      {{ if eq .RoleName "tool" -}}
      </tool_response>
      {{ end -}}
      {{ if .FunctionCall -}}
      <tool_call>
      {{toJson .FunctionCall}}
      </tool_call>
      {{ end -}}<|im_end|>
    function: |
      <|im_start|>system
      # Tools
      You may call one or more functions to assist with the user query.
      You are provided with function signatures within <tools></tools> XML tags:
      <tools>
      {{range .Functions}}
      {'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
      {{end}}
      </tools>
      For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
      <tool_call>
      {\"name\": <function-name>, \"arguments\": <args-json-object>}
      </tool_call>
      <|im_end|>
      {{.Input -}}
      <|im_start|>assistant
    chat: |
      {{.Input -}}
      <|im_start|>assistant
      <think>
    completion: |
      {{.Input}}
  context_size: 8192
  function:
    grammar:
      triggers:
      - word: "<tool_call>"
  f16: true
  stopwords:
  - '<|im_end|>'
  - '<dummy32000>'
  - '</s>'
  - '<|endoftext|>'
