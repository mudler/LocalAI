---
name: 'build backend container images'

on:
  pull_request:
  push:
    branches:
      - master
    tags:
      - '*'

concurrency:
  group: ci-backends-${{ github.head_ref || github.ref }}-${{ github.repository }}
  cancel-in-progress: true

jobs:
  backend-jobs:
    uses: ./.github/workflows/backend_build.yml
    with:
      tag-latest: ${{ matrix.tag-latest }}
      tag-suffix: ${{ matrix.tag-suffix }}
      build-type: ${{ matrix.build-type }}
      cuda-major-version: ${{ matrix.cuda-major-version }}
      cuda-minor-version: ${{ matrix.cuda-minor-version }}
      platforms: ${{ matrix.platforms }}
      runs-on: ${{ matrix.runs-on }}
      base-image: ${{ matrix.base-image }}
      backend: ${{ matrix.backend }}
      dockerfile: ${{ matrix.dockerfile }}
      skip-drivers: ${{ matrix.skip-drivers }}
      context: ${{ matrix.context }}
    secrets:
      dockerUsername: ${{ secrets.DOCKERHUB_USERNAME }}
      dockerPassword: ${{ secrets.DOCKERHUB_PASSWORD }}
      quayUsername: ${{ secrets.LOCALAI_REGISTRY_USERNAME }}
      quayPassword: ${{ secrets.LOCALAI_REGISTRY_PASSWORD }}
    strategy:
      fail-fast: false
      #max-parallel: ${{ github.event_name != 'pull_request' && 6 || 4 }}
      matrix:
        include:
          # # CUDA 11 builds
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-rerankers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "rerankers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-llama-cpp'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "llama-cpp"
          #   dockerfile: "./backend/Dockerfile.llama-cpp"
          #   context: "./"
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-transformers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "transformers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-diffusers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "diffusers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'l4t'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/arm64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-l4t-diffusers'
          #   runs-on: 'ubuntu-24.04-arm'
          #   base-image: "nvcr.io/nvidia/l4t-jetpack:r36.4.0"
          #   skip-drivers: 'true'
          #   backend: "diffusers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: ''
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-cpu-diffusers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'true'
          #   backend: "diffusers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: ''
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-cpu-chatterbox'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'true'
          #   backend: "chatterbox"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # CUDA 11 additional backends
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-kokoro'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "kokoro"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-faster-whisper'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "faster-whisper"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-coqui'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "coqui"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-bark'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "bark"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "11"
          #   cuda-minor-version: "7"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-11-chatterbox'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "chatterbox"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # CUDA 12 builds
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-rerankers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "rerankers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-llama-cpp'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "llama-cpp"
          #   dockerfile: "./backend/Dockerfile.llama-cpp"
          #   context: "./"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-vllm'
          #   runs-on: 'arc-runner-set'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "vllm"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-transformers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "transformers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-diffusers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "diffusers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # CUDA 12 additional backends
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-kokoro'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "kokoro"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-faster-whisper'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "faster-whisper"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-coqui'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "coqui"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-bark'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "bark"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-cuda-12-chatterbox'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "chatterbox"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # hipblas builds
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-rerankers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "rerankers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-llama-cpp'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "llama-cpp"
          #   dockerfile: "./backend/Dockerfile.llama-cpp"
          #   context: "./"
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-vllm'
          #   runs-on: 'arc-runner-set'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "vllm"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-transformers'
          #   runs-on: 'arc-runner-set'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "transformers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-diffusers'
          #   runs-on: 'arc-runner-set'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "diffusers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # ROCm additional backends
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-kokoro'
          #   runs-on: 'arc-runner-set'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "kokoro"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-faster-whisper'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "faster-whisper"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-coqui'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "coqui"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'hipblas'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-rocm-hipblas-bark'
          #   runs-on: 'arc-runner-set'
          #   base-image: "rocm/dev-ubuntu-22.04:6.4.3"
          #   skip-drivers: 'false'
          #   backend: "bark"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # sycl builds
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-rerankers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "rerankers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'sycl_f32'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-sycl-f32-llama-cpp'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "llama-cpp"
          #   dockerfile: "./backend/Dockerfile.llama-cpp"
          #   context: "./"
          # - build-type: 'sycl_f16'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-sycl-f16-llama-cpp'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "llama-cpp"
          #   dockerfile: "./backend/Dockerfile.llama-cpp"
          #   context: "./"
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-vllm'
          #   runs-on: 'arc-runner-set'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "vllm"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-transformers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "transformers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-diffusers'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "diffusers"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'l4t'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/arm64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-nvidia-l4t-kokoro'
          #   runs-on: 'ubuntu-24.04-arm'
          #   base-image: "nvcr.io/nvidia/l4t-jetpack:r36.4.0"
          #   skip-drivers: 'true'
          #   backend: "kokoro"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # SYCL additional backends
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-kokoro'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "kokoro"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-faster-whisper'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "faster-whisper"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-coqui'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "coqui"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # - build-type: 'intel'
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-gpu-intel-bark'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "quay.io/go-skynet/intel-oneapi-base:latest"
          #   skip-drivers: 'false'
          #   backend: "bark"
          #   dockerfile: "./backend/Dockerfile.python"
          #   context: "./backend"
          # # piper
          # - build-type: ''
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64,linux/arm64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-piper'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "piper"
          #   dockerfile: "./backend/Dockerfile.golang"
          #   context: "./"
          # # bark-cpp
          # - build-type: ''
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-bark-cpp'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "bark-cpp"
          #   dockerfile: "./backend/Dockerfile.golang"
          #   context: "./"
          # - build-type: ''
          #   cuda-major-version: ""
          #   cuda-minor-version: ""
          #   platforms: 'linux/amd64,linux/arm64'
          #   tag-latest: 'auto'
          #   tag-suffix: '-cpu-llama-cpp'
          #   runs-on: 'ubuntu-latest'
          #   base-image: "ubuntu:22.04"
          #   skip-drivers: 'false'
          #   backend: "llama-cpp"
          #   dockerfile: "./backend/Dockerfile.llama-cpp"
          #   context: "./"
          # - build-type: 'cublas'
          #   cuda-major-version: "12"
          #   cuda-minor-version: "0"
          #   platforms: 'linux/arm64'
          #   skip-drivers: 'true'
          #   tag-latest: 'auto'
          #   tag-suffix: '-nvidia-l4t-arm64-llama-cpp'
          #   base-image: "nvcr.io/nvidia/l4t-jetpack:r36.4.0"
          #   runs-on: 'ubuntu-24.04-arm'
          #   backend: "llama-cpp"
          #   dockerfile: "./backend/Dockerfile.llama-cpp"
          #   context: "./"
          - build-type: 'vulkan'
            cuda-major-version: ""
            cuda-minor-version: ""
            platforms: 'linux/amd64,linux/arm64'
            tag-latest: 'auto'
            tag-suffix: '-gpu-vulkan-llama-cpp'
            runs-on: 'ubuntu-latest'
            base-image: "ubuntu:22.04"
            skip-drivers: 'false'
            backend: "llama-cpp"
            dockerfile: "./backend/Dockerfile.llama-cpp"
            context: "./"
          - build-type: 'vulkan'
            cuda-major-version: ""
            cuda-minor-version: ""
            platforms: 'linux/amd64,linux/arm64'
            tag-latest: 'auto'
            tag-suffix: '-gpu-vulkan-stablediffusion-ggml'
            runs-on: 'ubuntu-latest'
            base-image: "ubuntu:22.04"
            skip-drivers: 'false'
            backend: "stablediffusion-ggml"
            dockerfile: "./backend/Dockerfile.golang"
            context: "./"
          - build-type: 'vulkan'
            cuda-major-version: ""
            cuda-minor-version: ""
            platforms: 'linux/amd64,linux/arm64'
            tag-latest: 'auto'
            tag-suffix: '-gpu-vulkan-whisper'
            runs-on: 'ubuntu-latest'
            base-image: "ubuntu:22.04"
            skip-drivers: 'false'
            backend: "whisper"
            dockerfile: "./backend/Dockerfile.golang"
            context: "./"
